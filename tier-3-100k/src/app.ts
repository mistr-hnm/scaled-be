/**
 * â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
 *  TIER 3 â€” 100,000 requests/second
 *  Stack: Cluster + Redis Cache + Read Replicas + PgBouncer
 *  Target: High-traffic production apps
 *  Hardware: 3+ app servers behind a load balancer
 * â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
 *
 *  NEW vs TIER 2:
 *  âœ… Redis caching â€” serve GET requests without hitting DB
 *  âœ… Read replica pool â€” writes go to primary, reads to replica
 *  âœ… PgBouncer â€” proxy that limits real DB connections (see config below)
 *  âœ… Cache-aside pattern â€” check Redis first, fallback to DB
 *  âœ… Cache invalidation on write â€” keep data consistent
 *  âœ… Cursor-based pagination â€” replaces slow OFFSET
 *  âœ… Request ID tracking â€” trace requests across services
 */

import express, { Request, Response, NextFunction } from "express";
import compression from "compression";
import helmet from "helmet";
import { randomUUID } from "crypto";
import pino from "pino";
import { cacheDel, cacheGet, cacheSet, redis, userKey, userListKey } from "./redis";
import { primaryPool, replicaPool } from "./db";

const logger = pino({ level: process.env.LOG_LEVEL || "info" });
 
const app = express();

app.use(helmet());
app.use(compression());
app.use(express.json({ limit: "100kb" }));

// Attach request ID for distributed tracing
app.use((req: Request, res: Response, next: NextFunction) => {
  const requestId = (req.headers["x-request-id"] as string) || randomUUID();
  res.setHeader("x-request-id", requestId);
  (req as any).requestId = requestId;
  next();
});

// routes
app.get("/health", async (_req, res) => {
  const [pgPrimary, pgReplica, redisOk] = await Promise.allSettled([
    primaryPool.query("SELECT 1"),
    replicaPool.query("SELECT 1"),
    redis.ping(),
  ]);

  res.json({
    ok: true,
    services: {
      db_primary: pgPrimary.status === "fulfilled" ? "ok" : "down",
      db_replica: pgReplica.status === "fulfilled" ? "ok" : "down",
      redis: redisOk.status === "fulfilled" ? "ok" : "down",
    },
  });
});

// â”€â”€â”€ LIST â€” cursor pagination (replaces slow OFFSET) â”€â”€â”€â”€â”€â”€
// OFFSET scans N rows every time. At page 10000, that's 200k rows scanned.
// Cursor pagination always scans from the last seen ID â€” O(1) regardless of depth.
app.get("/api/users", async (req, res, next) => {
  try {
    const cursor = Number(req.query.cursor) || 0; // last seen ID
    const limit  = Math.min(100, Number(req.query.limit) || 20);

    // Check Redis first
    const cacheKey = userListKey(cursor, limit);
    const cached = await cacheGet<User[]>(cacheKey);

    if (cached) {
      res.setHeader("x-cache", "HIT");
      return res.json({ data: cached, next_cursor: cached[cached.length - 1]?.id ?? null });
    }

    // Miss â†’ query REPLICA (not primary â€” reads don't need the freshest data)
    const { rows } = await replicaPool.query<User>(
      `SELECT id, name, email, created_at
       FROM users
       WHERE id > $1
       ORDER BY id ASC
       LIMIT $2`,
      [cursor, limit]
    );

    await cacheSet(cacheKey, rows, 30); // short TTL for list â€” changes frequently

    res.setHeader("x-cache", "MISS");
    res.json({
      data: rows,
      next_cursor: rows[rows.length - 1]?.id ?? null, // client passes this as ?cursor= next time
    });
  } catch (err) {
    next(err);
  }
});

// â”€â”€â”€ GET ONE â€” cache-aside pattern â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
app.get("/api/users/:id", async (req, res, next) => {
  try {
    const { id } = req.params;

    // 1. Check cache
    const cached = await cacheGet<User>(userKey(id));
    if (cached) {
      res.setHeader("x-cache", "HIT");
      return res.json({ data: cached });
    }

    // 2. Read from replica
    const { rows } = await replicaPool.query<User>(
      "SELECT id, name, email, created_at FROM users WHERE id=$1",
      [id]
    );
    if (!rows.length) return res.status(404).json({ error: "Not found" });

    // 3. Populate cache for next time
    await cacheSet(userKey(id), rows[0]);

    res.setHeader("x-cache", "MISS");
    res.json({ data: rows[0] });
  } catch (err) {
    next(err);
  }
});

// â”€â”€â”€ CREATE â€” write to primary, invalidate cache â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
app.post("/api/users", async (req, res, next) => {
  try {
    const { name, email } = req.body as { name: string; email: string };

    const { rows } = await primaryPool.query<User>(
      `INSERT INTO users (name, email)
       VALUES ($1, $2)
       RETURNING id, name, email, created_at`,
      [name, email]
    );

    // Invalidate list caches â€” they're now stale
    // (We use a pattern delete; in production use Redis SCAN or a cache tag system)
    await cacheDel(`users:list:0:20`); // invalidate most common first page

    res.status(201).json({ data: rows[0] });
  } catch (err: any) {
    if (err.code === "23505") return res.status(409).json({ error: "Email taken" });
    next(err);
  }
});

// â”€â”€â”€ UPDATE â€” write primary, invalidate specific user cache
app.put("/api/users/:id", async (req, res, next) => {
  try {
    const { name, email } = req.body as { name: string; email: string };
    const { id } = req.params;

    const { rows } = await primaryPool.query<User>(
      `UPDATE users SET name=$1, email=$2, updated_at=NOW()
       WHERE id=$3 RETURNING id, name, email, created_at`,
      [name, email, id]
    );

    if (!rows.length) return res.status(404).json({ error: "Not found" });

    // Invalidate this user's cache entry
    await cacheDel(userKey(id));

    res.json({ data: rows[0] });
  } catch (err) {
    next(err);
  }
});

// â”€â”€â”€ DELETE â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
app.delete("/api/users/:id", async (req, res, next) => {
  try {
    const { id } = req.params;
    const { rowCount } = await primaryPool.query(
      "DELETE FROM users WHERE id=$1",
      [id]
    );
    if (!rowCount) return res.status(404).json({ error: "Not found" });

    await cacheDel(userKey(id));

    res.status(204).send();
  } catch (err) {
    next(err);
  }
});

// Error handler
app.use((err: Error, _req: Request, res: Response, _next: NextFunction) => {
  logger.error({ err }, "Unhandled error");
  res.status(500).json({ error: "Internal server error" });
});

const PORT = Number(process.env.PORT) || 3000;
app.listen(PORT, () => logger.info(`Worker ${process.pid} on :${PORT}`));

/**
 * â”€â”€â”€ TIER 3 SUMMARY â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
 *
 *  âœ… What's new:
 *     - Redis cache: ~0.1ms reads vs ~5ms DB queries
 *     - Primary/Replica split: writes & reads are independent
 *     - PgBouncer: caps real DB connections at 100 (see pgbouncer.ini below)
 *     - Cursor pagination: O(1) regardless of depth
 *     - Cache invalidation on every write
 *
 *  ðŸ“Š Benchmark targets:
 *     - Latency p99 (cache hit):  < 5ms
 *     - Latency p99 (cache miss): < 30ms
 *     - Throughput: 100,000 req/s (across 3+ servers)
 *     - DB connections: ~100 real connections via PgBouncer
 *     - Servers: 3 app servers + load balancer + Redis + Postgres primary + replica
 *
 *  ðŸ”§ PgBouncer config (pgbouncer.ini):
 *
 *     [databases]
 *     postgres = host=postgres-primary port=5432 dbname=postgres
 *
 *     [pgbouncer]
 *     pool_mode = transaction        â† transaction-level pooling (most efficient)
 *     max_client_conn = 1000         â† accept up to 1000 client connections
 *     default_pool_size = 100        â† but only open 100 real Postgres connections
 *     reserve_pool_size = 10
 *
 *  ðŸ—ï¸  Infrastructure:
 *     nginx (load balancer)
 *       â”œâ”€â”€ App Server 1 (8 workers)
 *       â”œâ”€â”€ App Server 2 (8 workers)
 *       â””â”€â”€ App Server 3 (8 workers)
 *             â”‚
 *          PgBouncer
 *             â”œâ”€â”€ Postgres Primary  (writes)
 *             â””â”€â”€ Postgres Replica  (reads)
 *             Redis Cluster
 */
